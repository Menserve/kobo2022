{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d0b837",
   "metadata": {},
   "source": [
    "### 工房第４回レポート課題\n",
    "課題再掲：ニューラルネットワークの学習アルゴリズムをPython+Numpyを使って実装する。下記のサンプルコードを実行してMNISTデータを学習する。\n",
    "結果：20epochでテストデータの推論精度は96.4%であった。\n",
    "考察：前回課題の２層のニューラルネットワークのテストデータ推論精度は97.4%よりも低い精度となった。これは学習に使ったデータ量の差が影響したものと考えられる。すなわち、学習の手法が良くてもデータ量が不足していれば汎化性能は上がらないことを意味している。一方で、演算性能と十分なデータがあれば多少の学習手法の差はあっても、優れた学習モデルを構築できるということも示している。\n",
    "感想：正直なところ、CNNやその他学習効率を上げる手法を十分に理解したとは言えないがPytorch学習を進めながら理解を深めたいと思う。Pytorchによる環境設定は、M1 Macbookは少し特殊な設定なようだったが完了した（おそらく）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad7258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3001650301786567\n",
      "=== epoch:1, train acc:0.319, test acc:0.32 ===\n",
      "train loss:2.2974395981285616\n",
      "train loss:2.292348570857784\n",
      "train loss:2.2839533850430924\n",
      "train loss:2.2734563129001883\n",
      "train loss:2.2667523152285955\n",
      "train loss:2.256169388244361\n",
      "train loss:2.2205037042698264\n",
      "train loss:2.1960293691638966\n",
      "train loss:2.176144612626978\n",
      "train loss:2.1357837101881634\n",
      "train loss:2.0840597709275226\n",
      "train loss:2.032054221501896\n",
      "train loss:1.973404124605487\n",
      "train loss:1.9045494201221416\n",
      "train loss:1.848209371527985\n",
      "train loss:1.8667069948525599\n",
      "train loss:1.750981247287001\n",
      "train loss:1.6308824537664974\n",
      "train loss:1.5427039496960853\n",
      "train loss:1.4573715179604259\n",
      "train loss:1.3327434378280507\n",
      "train loss:1.166707173954189\n",
      "train loss:1.1838451292366292\n",
      "train loss:1.1098013224781418\n",
      "train loss:1.1369929336792501\n",
      "train loss:1.0313608189125234\n",
      "train loss:0.922001338332602\n",
      "train loss:0.8371551298523704\n",
      "train loss:0.9525517536299469\n",
      "train loss:0.8597583162316752\n",
      "train loss:0.7460008945372499\n",
      "train loss:0.890632443107569\n",
      "train loss:0.813570542694273\n",
      "train loss:0.6037112243160604\n",
      "train loss:0.7290004719542753\n",
      "train loss:0.48702606520831027\n",
      "train loss:0.580236659714297\n",
      "train loss:0.5754335692937256\n",
      "train loss:0.5413672272603691\n",
      "train loss:0.6911677323484521\n",
      "train loss:0.882004959121451\n",
      "train loss:0.7732193081407533\n",
      "train loss:0.47996745212746217\n",
      "train loss:0.5128580615267899\n",
      "train loss:0.4748699082767719\n",
      "train loss:0.6973436337287146\n",
      "train loss:0.5825020821244888\n",
      "train loss:0.4020593654058742\n",
      "train loss:0.44256943726143405\n",
      "train loss:0.41222723989574567\n",
      "=== epoch:2, train acc:0.817, test acc:0.813 ===\n",
      "train loss:0.5965606508324641\n",
      "train loss:0.5381782373577804\n",
      "train loss:0.5622878683773603\n",
      "train loss:0.3926319867395432\n",
      "train loss:0.5515154522926027\n",
      "train loss:0.36910826257607016\n",
      "train loss:0.5255215667064799\n",
      "train loss:0.43271831414019135\n",
      "train loss:0.5005379778138981\n",
      "train loss:0.503317909271483\n",
      "train loss:0.45972566780765484\n",
      "train loss:0.3717922428068446\n",
      "train loss:0.4498545981549109\n",
      "train loss:0.476180041742273\n",
      "train loss:0.39477658776794994\n",
      "train loss:0.4978147293181208\n",
      "train loss:0.46819211121285237\n",
      "train loss:0.35150513274557227\n",
      "train loss:0.5293100042874713\n",
      "train loss:0.4105532900265746\n",
      "train loss:0.4135145258470363\n",
      "train loss:0.35178737577255853\n",
      "train loss:0.3979702597999884\n",
      "train loss:0.3412638918121993\n",
      "train loss:0.47709646578793646\n",
      "train loss:0.45194520386212084\n",
      "train loss:0.4485937392105078\n",
      "train loss:0.38555306048709004\n",
      "train loss:0.49754436186401557\n",
      "train loss:0.6143493748858656\n",
      "train loss:0.40446701347331093\n",
      "train loss:0.43444742070372805\n",
      "train loss:0.40184686406948117\n",
      "train loss:0.3281946405545262\n",
      "train loss:0.4585591852584042\n",
      "train loss:0.47802795280400295\n",
      "train loss:0.5478344882161628\n",
      "train loss:0.32239860872708753\n",
      "train loss:0.3347698148117151\n",
      "train loss:0.29551913187672557\n",
      "train loss:0.32830394256627393\n",
      "train loss:0.3615002475285321\n",
      "train loss:0.3748537629380299\n",
      "train loss:0.34758510907506135\n",
      "train loss:0.41820882556181893\n",
      "train loss:0.4442476697561044\n",
      "train loss:0.5013326541217067\n",
      "train loss:0.2811487488958449\n",
      "train loss:0.3357879510505903\n",
      "train loss:0.30045158752489276\n",
      "=== epoch:3, train acc:0.878, test acc:0.865 ===\n",
      "train loss:0.41065007938729986\n",
      "train loss:0.35068874414935414\n",
      "train loss:0.26741833285844385\n",
      "train loss:0.43033214187430474\n",
      "train loss:0.33737081700807564\n",
      "train loss:0.2030003794302263\n",
      "train loss:0.3065569926364459\n",
      "train loss:0.3448605761191032\n",
      "train loss:0.28917008123601134\n",
      "train loss:0.28048277759749896\n",
      "train loss:0.3112843496504455\n",
      "train loss:0.24983530494947057\n",
      "train loss:0.4014501765928146\n",
      "train loss:0.3136328367737882\n",
      "train loss:0.18230339216822547\n",
      "train loss:0.3438139354066372\n",
      "train loss:0.37882815302246764\n",
      "train loss:0.2971324534654897\n",
      "train loss:0.23174863519634475\n",
      "train loss:0.31522844359461677\n",
      "train loss:0.3908038789301012\n",
      "train loss:0.3345873410638548\n",
      "train loss:0.3146887213778798\n",
      "train loss:0.24878992220128718\n",
      "train loss:0.3285801623458766\n",
      "train loss:0.325074288750281\n",
      "train loss:0.3292572092907622\n",
      "train loss:0.2879118810538359\n",
      "train loss:0.2647543342813504\n",
      "train loss:0.3314722971528181\n",
      "train loss:0.21316417051265885\n",
      "train loss:0.2582585807791666\n",
      "train loss:0.30928952871387877\n",
      "train loss:0.3968391774501772\n",
      "train loss:0.1401491188156385\n",
      "train loss:0.2456625667202214\n",
      "train loss:0.19926418064584514\n",
      "train loss:0.1863143029188654\n",
      "train loss:0.21281524857067438\n",
      "train loss:0.37394044629426704\n",
      "train loss:0.30078400040166786\n",
      "train loss:0.1695144149353294\n",
      "train loss:0.20493407826572135\n",
      "train loss:0.25009535057552146\n",
      "train loss:0.2574263942141135\n",
      "train loss:0.255910869847521\n",
      "train loss:0.37836561936003127\n",
      "train loss:0.2766042873247114\n",
      "train loss:0.15646208048818022\n",
      "train loss:0.3707610156770582\n",
      "=== epoch:4, train acc:0.907, test acc:0.883 ===\n",
      "train loss:0.3951270488314427\n",
      "train loss:0.3091888180920666\n",
      "train loss:0.23421293603968898\n",
      "train loss:0.45023483672385667\n",
      "train loss:0.24433152982333528\n",
      "train loss:0.23688100343252194\n",
      "train loss:0.24033465499679965\n",
      "train loss:0.21461012335138147\n",
      "train loss:0.265386280215705\n",
      "train loss:0.20044373620391592\n",
      "train loss:0.3494648606476238\n",
      "train loss:0.2774257848463668\n",
      "train loss:0.2523518739754376\n",
      "train loss:0.3659415422967296\n",
      "train loss:0.2669032790682398\n",
      "train loss:0.21365316310428978\n",
      "train loss:0.2543050777598959\n",
      "train loss:0.1727591875931627\n",
      "train loss:0.33149963463462406\n",
      "train loss:0.22163186795839043\n",
      "train loss:0.6776802350813772\n",
      "train loss:0.44965230591184313\n",
      "train loss:0.3046526976126887\n",
      "train loss:0.2912698965464628\n",
      "train loss:0.28156884292831985\n",
      "train loss:0.20589756481877985\n",
      "train loss:0.19993896043199277\n",
      "train loss:0.2810596627939192\n",
      "train loss:0.29740323770973204\n",
      "train loss:0.21617470958946763\n",
      "train loss:0.21441666497731382\n",
      "train loss:0.2065441861224068\n",
      "train loss:0.38027530229448686\n",
      "train loss:0.3200963746638707\n",
      "train loss:0.20790068389496505\n",
      "train loss:0.29171289914950266\n",
      "train loss:0.3738327594015698\n",
      "train loss:0.19291531329888364\n",
      "train loss:0.19244706380585166\n",
      "train loss:0.24020368337096212\n",
      "train loss:0.32746807866919847\n",
      "train loss:0.19399285063691926\n",
      "train loss:0.16882066880971164\n",
      "train loss:0.18193147234210025\n",
      "train loss:0.20567549350755876\n",
      "train loss:0.20569558119967\n",
      "train loss:0.3697272347075088\n",
      "train loss:0.16982478821015026\n",
      "train loss:0.12136111271130581\n",
      "train loss:0.14027317792675262\n",
      "=== epoch:5, train acc:0.915, test acc:0.901 ===\n",
      "train loss:0.2383070090167022\n",
      "train loss:0.15112438274024817\n",
      "train loss:0.1474412339001619\n",
      "train loss:0.3876682956455453\n",
      "train loss:0.277937523379122\n",
      "train loss:0.17729851454945564\n",
      "train loss:0.2748296135033886\n",
      "train loss:0.2013753800431991\n",
      "train loss:0.18456539522806145\n",
      "train loss:0.15091226950581296\n",
      "train loss:0.25366440707718074\n",
      "train loss:0.1317357242458214\n",
      "train loss:0.13874928677052112\n",
      "train loss:0.22329729165413706\n",
      "train loss:0.18515502046929758\n",
      "train loss:0.146974851759492\n",
      "train loss:0.27748908823817237\n",
      "train loss:0.1601374083120776\n",
      "train loss:0.1929277436249404\n",
      "train loss:0.23382163401372297\n",
      "train loss:0.23877103541866712\n",
      "train loss:0.19083313683782094\n",
      "train loss:0.12337642373861507\n",
      "train loss:0.21446526348525313\n",
      "train loss:0.1561914937844458\n",
      "train loss:0.20818343703129036\n",
      "train loss:0.35876526015172694\n",
      "train loss:0.4014004681271332\n",
      "train loss:0.17097263316420147\n",
      "train loss:0.38325370686576243\n",
      "train loss:0.34714162423266354\n",
      "train loss:0.22804399335633888\n",
      "train loss:0.15362314285375148\n",
      "train loss:0.2367700819212046\n",
      "train loss:0.2714565070331226\n",
      "train loss:0.12404627984533036\n",
      "train loss:0.2105193369297909\n",
      "train loss:0.23543449651007617\n",
      "train loss:0.26974591065901043\n",
      "train loss:0.18498075423145013\n",
      "train loss:0.2660225867598874\n",
      "train loss:0.1583458147714226\n",
      "train loss:0.12977667098892864\n",
      "train loss:0.20856664604159167\n",
      "train loss:0.19762014769515093\n",
      "train loss:0.2486253567280702\n",
      "train loss:0.1599233160117015\n",
      "train loss:0.15184735280137313\n",
      "train loss:0.20803507537715205\n",
      "train loss:0.1872494175691118\n",
      "=== epoch:6, train acc:0.928, test acc:0.909 ===\n",
      "train loss:0.2727828005224167\n",
      "train loss:0.21089078399225392\n",
      "train loss:0.10036636364648356\n",
      "train loss:0.15328300220644722\n",
      "train loss:0.2933346044259126\n",
      "train loss:0.174813648992166\n",
      "train loss:0.2699982194360861\n",
      "train loss:0.14899354706247955\n",
      "train loss:0.1314681621605302\n",
      "train loss:0.16120796195433498\n",
      "train loss:0.26395819516238195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2410476452180961\n",
      "train loss:0.2289000773612986\n",
      "train loss:0.1287982304586967\n",
      "train loss:0.165593717119858\n",
      "train loss:0.11785179561013465\n",
      "train loss:0.1220206917326574\n",
      "train loss:0.24220007472989052\n",
      "train loss:0.18249171322255336\n",
      "train loss:0.1730338904487333\n",
      "train loss:0.11227866468107271\n",
      "train loss:0.32074013124066786\n",
      "train loss:0.1275486682060172\n",
      "train loss:0.18824769950904552\n",
      "train loss:0.28234669618288516\n",
      "train loss:0.25412401486257524\n",
      "train loss:0.1527683510797834\n",
      "train loss:0.25920400009161026\n",
      "train loss:0.09422338259935716\n",
      "train loss:0.11359883405503617\n",
      "train loss:0.15265813033012576\n",
      "train loss:0.1631061460948387\n",
      "train loss:0.18517990425463765\n",
      "train loss:0.14827027531679363\n",
      "train loss:0.1667268058838101\n",
      "train loss:0.1030650285486598\n",
      "train loss:0.24834953554011768\n",
      "train loss:0.17173104614675394\n",
      "train loss:0.12550766735100038\n",
      "train loss:0.13305852943489999\n",
      "train loss:0.10249939808703921\n",
      "train loss:0.08285915970588728\n",
      "train loss:0.218367537377669\n",
      "train loss:0.1832047156010546\n",
      "train loss:0.23092940705846626\n",
      "train loss:0.24918661873037554\n",
      "train loss:0.11961091880312948\n",
      "train loss:0.07856338588982982\n",
      "train loss:0.09755063631975693\n",
      "train loss:0.1514881369176233\n",
      "=== epoch:7, train acc:0.927, test acc:0.912 ===\n",
      "train loss:0.09860429412686042\n",
      "train loss:0.16176417247510144\n",
      "train loss:0.12256640841152963\n",
      "train loss:0.2204446944211919\n",
      "train loss:0.09883710906802974\n",
      "train loss:0.19125347079592991\n",
      "train loss:0.1433063459462981\n",
      "train loss:0.18485403706417236\n",
      "train loss:0.19146308617259458\n",
      "train loss:0.12414054594526147\n",
      "train loss:0.15719785124174157\n",
      "train loss:0.10090746135419032\n",
      "train loss:0.22793503230179846\n",
      "train loss:0.21641441400057485\n",
      "train loss:0.10925222793664269\n",
      "train loss:0.18970409876586217\n",
      "train loss:0.152129840991302\n",
      "train loss:0.14766475282927163\n",
      "train loss:0.20316678993905574\n",
      "train loss:0.20082855018670778\n",
      "train loss:0.20361809524451932\n",
      "train loss:0.17982059137222603\n",
      "train loss:0.12365673970201854\n",
      "train loss:0.11730392032885209\n",
      "train loss:0.11742875092971598\n",
      "train loss:0.07067514461663091\n",
      "train loss:0.05680251772408809\n",
      "train loss:0.17304389667337172\n",
      "train loss:0.28799540472694773\n",
      "train loss:0.11942793697829382\n",
      "train loss:0.2750283002160811\n",
      "train loss:0.19148191572330983\n",
      "train loss:0.11919005149935355\n",
      "train loss:0.20614490229202784\n",
      "train loss:0.078328182364149\n",
      "train loss:0.11825701555712685\n",
      "train loss:0.40848527542433727\n",
      "train loss:0.11425478950953012\n",
      "train loss:0.1883562475696559\n",
      "train loss:0.07641895031623645\n",
      "train loss:0.19924172137273025\n",
      "train loss:0.10788021031796818\n",
      "train loss:0.20387824710605848\n",
      "train loss:0.2179287088281419\n",
      "train loss:0.1258662988115943\n",
      "train loss:0.17442218068182241\n",
      "train loss:0.27758595704876166\n",
      "train loss:0.1624604882899642\n",
      "train loss:0.21063852654982315\n",
      "train loss:0.2111857962458699\n",
      "=== epoch:8, train acc:0.946, test acc:0.935 ===\n",
      "train loss:0.1676699647051081\n",
      "train loss:0.1538057960932304\n",
      "train loss:0.0426283738870974\n",
      "train loss:0.20202364435319134\n",
      "train loss:0.11040032753480007\n",
      "train loss:0.15250765056096055\n",
      "train loss:0.19433170440830783\n",
      "train loss:0.06441865667514879\n",
      "train loss:0.08084972405105795\n",
      "train loss:0.13772266330005553\n",
      "train loss:0.17063396635109776\n",
      "train loss:0.11042104036852018\n",
      "train loss:0.14626802911050724\n",
      "train loss:0.17369681307901289\n",
      "train loss:0.17621423687742477\n",
      "train loss:0.19488229339299395\n",
      "train loss:0.13343039785690988\n",
      "train loss:0.10866393302930467\n",
      "train loss:0.08114667933704765\n",
      "train loss:0.13508615989500514\n",
      "train loss:0.20965158985504784\n",
      "train loss:0.10398688821953106\n",
      "train loss:0.14897743190989113\n",
      "train loss:0.22277576108123587\n",
      "train loss:0.09611271508217017\n",
      "train loss:0.14198810513443466\n",
      "train loss:0.12932531029190947\n",
      "train loss:0.15346188278759235\n",
      "train loss:0.21876306957347516\n",
      "train loss:0.1512660694454522\n",
      "train loss:0.06933496408867856\n",
      "train loss:0.1208594437314224\n",
      "train loss:0.09027543927488475\n",
      "train loss:0.22712175771328674\n",
      "train loss:0.11926286480837628\n",
      "train loss:0.10533829016936265\n",
      "train loss:0.10855922966096802\n",
      "train loss:0.2112319152251123\n",
      "train loss:0.1307202992506822\n",
      "train loss:0.21367232418493134\n",
      "train loss:0.12953377025007268\n",
      "train loss:0.13855655474898632\n",
      "train loss:0.10133051728805846\n",
      "train loss:0.0971065526625166\n",
      "train loss:0.09566925115297355\n",
      "train loss:0.05926434028137883\n",
      "train loss:0.16743265587896405\n",
      "train loss:0.2656004704098956\n",
      "train loss:0.10276150453117898\n",
      "train loss:0.12904483419677587\n",
      "=== epoch:9, train acc:0.952, test acc:0.931 ===\n",
      "train loss:0.1349645292024942\n",
      "train loss:0.1147752336290918\n",
      "train loss:0.12067117619176701\n",
      "train loss:0.18341908234079063\n",
      "train loss:0.1351805810074354\n",
      "train loss:0.10504495301534736\n",
      "train loss:0.0812418365969231\n",
      "train loss:0.05599601383773536\n",
      "train loss:0.11069015034259153\n",
      "train loss:0.10817776172571614\n",
      "train loss:0.12538483397479327\n",
      "train loss:0.10256563121238994\n",
      "train loss:0.055341111685364985\n",
      "train loss:0.11066368819733682\n",
      "train loss:0.07269377019579604\n",
      "train loss:0.25977333478375053\n",
      "train loss:0.12905736773433976\n",
      "train loss:0.13027610564866735\n",
      "train loss:0.10170804214337908\n",
      "train loss:0.10838459892746372\n",
      "train loss:0.09217094311842378\n",
      "train loss:0.12321542710424405\n",
      "train loss:0.061095631875547424\n",
      "train loss:0.06312759418807597\n",
      "train loss:0.09270316850952856\n",
      "train loss:0.12549817227567656\n",
      "train loss:0.11299971322521189\n",
      "train loss:0.09030735049477669\n",
      "train loss:0.12708044903773744\n",
      "train loss:0.1141062104866845\n",
      "train loss:0.08212112467523547\n",
      "train loss:0.07428576655349983\n",
      "train loss:0.11406693500434001\n",
      "train loss:0.23988501776092958\n",
      "train loss:0.13024323416758965\n",
      "train loss:0.08353534269864199\n",
      "train loss:0.06828556959056689\n",
      "train loss:0.05257290743122828\n",
      "train loss:0.10531228686526145\n",
      "train loss:0.10737902257922109\n",
      "train loss:0.10767849586944397\n",
      "train loss:0.1986294974225779\n",
      "train loss:0.07885409445903227\n",
      "train loss:0.11961729667471217\n",
      "train loss:0.064386919684786\n",
      "train loss:0.07455894903394224\n",
      "train loss:0.06933836573200128\n",
      "train loss:0.10620009853355052\n",
      "train loss:0.07111590985649002\n",
      "train loss:0.10914301437246036\n",
      "=== epoch:10, train acc:0.96, test acc:0.937 ===\n",
      "train loss:0.06541422511116518\n",
      "train loss:0.05306481983619944\n",
      "train loss:0.08983815389197812\n",
      "train loss:0.08868932096041326\n",
      "train loss:0.15746753262871377\n",
      "train loss:0.14728215846092307\n",
      "train loss:0.09448005538861558\n",
      "train loss:0.09970831204175475\n",
      "train loss:0.08312850834305069\n",
      "train loss:0.14948622261138098\n",
      "train loss:0.12781192889326007\n",
      "train loss:0.13348284289290327\n",
      "train loss:0.07048488163675666\n",
      "train loss:0.08957105680418856\n",
      "train loss:0.06674369303950459\n",
      "train loss:0.2320127952960204\n",
      "train loss:0.09090296019670192\n",
      "train loss:0.036658359985438996\n",
      "train loss:0.12276165361410481\n",
      "train loss:0.028786946886733576\n",
      "train loss:0.09479213295645957\n",
      "train loss:0.07433400794667204\n",
      "train loss:0.083251999113974\n",
      "train loss:0.09386916034703247\n",
      "train loss:0.1001645998655411\n",
      "train loss:0.09605420467710322\n",
      "train loss:0.10712189754357486\n",
      "train loss:0.0989846797877867\n",
      "train loss:0.09024966860735675\n",
      "train loss:0.10711416674592168\n",
      "train loss:0.08957510349716452\n",
      "train loss:0.10551205538215498\n",
      "train loss:0.10473477234805767\n",
      "train loss:0.07415639893103931\n",
      "train loss:0.09824986846489278\n",
      "train loss:0.18207855378297694\n",
      "train loss:0.14650508566182516\n",
      "train loss:0.13660449042589096\n",
      "train loss:0.032542096327619174\n",
      "train loss:0.058407579614749475\n",
      "train loss:0.1033979478707579\n",
      "train loss:0.05876789474008604\n",
      "train loss:0.08255845697509645\n",
      "train loss:0.03139434256892696\n",
      "train loss:0.07451396292574915\n",
      "train loss:0.03887311549233924\n",
      "train loss:0.04039430659432257\n",
      "train loss:0.13990091340231312\n",
      "train loss:0.039486001994070714\n",
      "train loss:0.1000196766018323\n",
      "=== epoch:11, train acc:0.96, test acc:0.942 ===\n",
      "train loss:0.08911724700539919\n",
      "train loss:0.0796958169714126\n",
      "train loss:0.17394825702314662\n",
      "train loss:0.08434587068411073\n",
      "train loss:0.10552576611437711\n",
      "train loss:0.12915094774971164\n",
      "train loss:0.14555914356655061\n",
      "train loss:0.09038906330392928\n",
      "train loss:0.10464148141144328\n",
      "train loss:0.10547662494169213\n",
      "train loss:0.02731770768999251\n",
      "train loss:0.10062202716556716\n",
      "train loss:0.06882945717262552\n",
      "train loss:0.03009206931453401\n",
      "train loss:0.05790000913239662\n",
      "train loss:0.04544399752149135\n",
      "train loss:0.07368053285302692\n",
      "train loss:0.116309831933718\n",
      "train loss:0.05294852309439299\n",
      "train loss:0.06071899429192269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1935444868086362\n",
      "train loss:0.14594720230966124\n",
      "train loss:0.03192943816349194\n",
      "train loss:0.048019543284368395\n",
      "train loss:0.07093981945888891\n",
      "train loss:0.08787086118058626\n",
      "train loss:0.05574365436448613\n",
      "train loss:0.034141882229424415\n",
      "train loss:0.030603129110434654\n",
      "train loss:0.053758272747950156\n",
      "train loss:0.04909090449358997\n",
      "train loss:0.13927994008278125\n",
      "train loss:0.08138375482435599\n",
      "train loss:0.03391360726745984\n",
      "train loss:0.07051286753049904\n",
      "train loss:0.053760288046084\n",
      "train loss:0.049737981849962865\n",
      "train loss:0.07443259707270068\n",
      "train loss:0.09976954409168252\n",
      "train loss:0.06381066490271912\n",
      "train loss:0.1664207486807341\n",
      "train loss:0.1657638966355998\n",
      "train loss:0.10448300833213396\n",
      "train loss:0.11543129413806734\n",
      "train loss:0.053879535757505786\n",
      "train loss:0.09759800520502626\n",
      "train loss:0.072086233852476\n",
      "train loss:0.04543555578195109\n",
      "train loss:0.060793737157634774\n",
      "train loss:0.042417934071322286\n",
      "=== epoch:12, train acc:0.967, test acc:0.942 ===\n",
      "train loss:0.05414331687095471\n",
      "train loss:0.04180966398873738\n",
      "train loss:0.05705352100383329\n",
      "train loss:0.0850699714877322\n",
      "train loss:0.05960672152822366\n",
      "train loss:0.1561015199853838\n",
      "train loss:0.06786808223728634\n",
      "train loss:0.05264587855591159\n",
      "train loss:0.03928608357898159\n",
      "train loss:0.04705118198617704\n",
      "train loss:0.06389490990874452\n",
      "train loss:0.023470796181697295\n",
      "train loss:0.10215852629365839\n",
      "train loss:0.03401239742922228\n",
      "train loss:0.14402313995537466\n",
      "train loss:0.055565754833158704\n",
      "train loss:0.1027302429947199\n",
      "train loss:0.062998696669645\n",
      "train loss:0.047356818692281345\n",
      "train loss:0.1283872709914869\n",
      "train loss:0.07519817867950757\n",
      "train loss:0.05631591438866681\n",
      "train loss:0.04766981270263038\n",
      "train loss:0.07395806246395449\n",
      "train loss:0.03807898033718502\n",
      "train loss:0.043955226137697784\n",
      "train loss:0.106389902748098\n",
      "train loss:0.05607535867030943\n",
      "train loss:0.0820421775082676\n",
      "train loss:0.07983417693174792\n",
      "train loss:0.04588394748569618\n",
      "train loss:0.06522275826296946\n",
      "train loss:0.08057240526026176\n",
      "train loss:0.037336192697768375\n",
      "train loss:0.13104842860428456\n",
      "train loss:0.06105336623025237\n",
      "train loss:0.06662987686205138\n",
      "train loss:0.03540167555172827\n",
      "train loss:0.14191704249228196\n",
      "train loss:0.07718253073954404\n",
      "train loss:0.035612673246541625\n",
      "train loss:0.03679012885821095\n",
      "train loss:0.06503991315623994\n",
      "train loss:0.08188102964367207\n",
      "train loss:0.020641586154485796\n",
      "train loss:0.0982256332184772\n",
      "train loss:0.121071776633879\n",
      "train loss:0.04994276307225056\n",
      "train loss:0.05339905710517451\n",
      "train loss:0.0623703404420785\n",
      "=== epoch:13, train acc:0.972, test acc:0.955 ===\n",
      "train loss:0.024748101836391468\n",
      "train loss:0.040471570501875484\n",
      "train loss:0.03940196474851641\n",
      "train loss:0.06331957621069248\n",
      "train loss:0.05828580287786525\n",
      "train loss:0.03638594263017791\n",
      "train loss:0.032098411333090125\n",
      "train loss:0.11524114367087268\n",
      "train loss:0.024320065097285938\n",
      "train loss:0.07051036017322658\n",
      "train loss:0.07897433815667995\n",
      "train loss:0.03845466514384041\n",
      "train loss:0.02877905617394393\n",
      "train loss:0.07129001061357422\n",
      "train loss:0.02347072923653347\n",
      "train loss:0.04890218907649295\n",
      "train loss:0.1501094701687841\n",
      "train loss:0.051376421619943484\n",
      "train loss:0.08021866568221686\n",
      "train loss:0.031899713928288\n",
      "train loss:0.039847499035960035\n",
      "train loss:0.07109418195142446\n",
      "train loss:0.07436828724201457\n",
      "train loss:0.06545282005271466\n",
      "train loss:0.055086076702337256\n",
      "train loss:0.06467135688178055\n",
      "train loss:0.052893053702622865\n",
      "train loss:0.029998983406030667\n",
      "train loss:0.16181676002891077\n",
      "train loss:0.05863481909297559\n",
      "train loss:0.06068491692378219\n",
      "train loss:0.02286481731519877\n",
      "train loss:0.10000102243424648\n",
      "train loss:0.03733404578320336\n",
      "train loss:0.07187242570333853\n",
      "train loss:0.07735778891596314\n",
      "train loss:0.07728480423471654\n",
      "train loss:0.04166273665937527\n",
      "train loss:0.05957678420995886\n",
      "train loss:0.04438537885756309\n",
      "train loss:0.05792248024294223\n",
      "train loss:0.05532348868041443\n",
      "train loss:0.05943229549369392\n",
      "train loss:0.14569143533932782\n",
      "train loss:0.07272403235247764\n",
      "train loss:0.032371543575610555\n",
      "train loss:0.07920228625543385\n",
      "train loss:0.05061965277132147\n",
      "train loss:0.04438118637457968\n",
      "train loss:0.03138281046776645\n",
      "=== epoch:14, train acc:0.978, test acc:0.95 ===\n",
      "train loss:0.021112569389027565\n",
      "train loss:0.04855780582267804\n",
      "train loss:0.030533642063809215\n",
      "train loss:0.05707265542055577\n",
      "train loss:0.04044081619252691\n",
      "train loss:0.08674457210780596\n",
      "train loss:0.06231832446217896\n",
      "train loss:0.02422159037027825\n",
      "train loss:0.05050840599265388\n",
      "train loss:0.08898219674222917\n",
      "train loss:0.025353590813428705\n",
      "train loss:0.03134737262985672\n",
      "train loss:0.04487302905691998\n",
      "train loss:0.035536413865217754\n",
      "train loss:0.018820772252878953\n",
      "train loss:0.03354644533299412\n",
      "train loss:0.02307909831456891\n",
      "train loss:0.03884411870832545\n",
      "train loss:0.0224897337697633\n",
      "train loss:0.07260421037466284\n",
      "train loss:0.0501537683696839\n",
      "train loss:0.09470664531345165\n",
      "train loss:0.050660363907072944\n",
      "train loss:0.07878839104868188\n",
      "train loss:0.028850599837681724\n",
      "train loss:0.03886966748473533\n",
      "train loss:0.03976809119021251\n",
      "train loss:0.07768845918704616\n",
      "train loss:0.01915555251138983\n",
      "train loss:0.026593436245416796\n",
      "train loss:0.011128265263980064\n",
      "train loss:0.022555809148905755\n",
      "train loss:0.04815433120121519\n",
      "train loss:0.05164653725615454\n",
      "train loss:0.0477321498682441\n",
      "train loss:0.022631572349497652\n",
      "train loss:0.06890062761121721\n",
      "train loss:0.11372295615512705\n",
      "train loss:0.01678687977390038\n",
      "train loss:0.06290620887033399\n",
      "train loss:0.01666610192208198\n",
      "train loss:0.02164503592551962\n",
      "train loss:0.08448248033946441\n",
      "train loss:0.031831253015906384\n",
      "train loss:0.14019014911423947\n",
      "train loss:0.036809623338921946\n",
      "train loss:0.05621751659802143\n",
      "train loss:0.08780532534924147\n",
      "train loss:0.06661109504567783\n",
      "train loss:0.052354748089146766\n",
      "=== epoch:15, train acc:0.983, test acc:0.96 ===\n",
      "train loss:0.054738797348848546\n",
      "train loss:0.06473183763569659\n",
      "train loss:0.02010462211807061\n",
      "train loss:0.0466830646040442\n",
      "train loss:0.0447958884356143\n",
      "train loss:0.050584061131659495\n",
      "train loss:0.04434680350898976\n",
      "train loss:0.04784054824337787\n",
      "train loss:0.10632838398781182\n",
      "train loss:0.026862174137616237\n",
      "train loss:0.02593482279242456\n",
      "train loss:0.019618634703517355\n",
      "train loss:0.026220510929382643\n",
      "train loss:0.038861235831324906\n",
      "train loss:0.05531004389316804\n",
      "train loss:0.09508274725559587\n",
      "train loss:0.0585510950254952\n",
      "train loss:0.05983453032735452\n",
      "train loss:0.03843533515680861\n",
      "train loss:0.04393231491573682\n",
      "train loss:0.06905026353877333\n",
      "train loss:0.03215443622878175\n",
      "train loss:0.029054551569785488\n",
      "train loss:0.04384307437400161\n",
      "train loss:0.025778426386865255\n",
      "train loss:0.04050902285151168\n",
      "train loss:0.07388535977947192\n",
      "train loss:0.039918945393133426\n",
      "train loss:0.034169032219487395\n",
      "train loss:0.06462685270794992\n",
      "train loss:0.04016097463978162\n",
      "train loss:0.031014289897268305\n",
      "train loss:0.17518761505776248\n",
      "train loss:0.028145566976524785\n",
      "train loss:0.02438032084833911\n",
      "train loss:0.0394879640425879\n",
      "train loss:0.06796227494548003\n",
      "train loss:0.016243090723560755\n",
      "train loss:0.03050193755016285\n",
      "train loss:0.03531444781297279\n",
      "train loss:0.059361114536849745\n",
      "train loss:0.0648413540062283\n",
      "train loss:0.015720617513242482\n",
      "train loss:0.06340890854380268\n",
      "train loss:0.046215554929240295\n",
      "train loss:0.05334225258334107\n",
      "train loss:0.01938256744641041\n",
      "train loss:0.025752617655292104\n",
      "train loss:0.06348980412219034\n",
      "train loss:0.01962316522751044\n",
      "=== epoch:16, train acc:0.984, test acc:0.956 ===\n",
      "train loss:0.06836529751360759\n",
      "train loss:0.018475084040547995\n",
      "train loss:0.06289626370943507\n",
      "train loss:0.02323123686800404\n",
      "train loss:0.030312580011744215\n",
      "train loss:0.043253537075248476\n",
      "train loss:0.046059844717430735\n",
      "train loss:0.016790154048249236\n",
      "train loss:0.024839871153108314\n",
      "train loss:0.03543410648684449\n",
      "train loss:0.050882552287938844\n",
      "train loss:0.029477196933080978\n",
      "train loss:0.03410775074443262\n",
      "train loss:0.023570561477091117\n",
      "train loss:0.09305927529266719\n",
      "train loss:0.024816879436812652\n",
      "train loss:0.0675618659217312\n",
      "train loss:0.12248554156575411\n",
      "train loss:0.021242925614210463\n",
      "train loss:0.030214248678662306\n",
      "train loss:0.043190189844175636\n",
      "train loss:0.04133953422604463\n",
      "train loss:0.05292289598207314\n",
      "train loss:0.04259499604019145\n",
      "train loss:0.033835347782887756\n",
      "train loss:0.05128181454158605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04219668673826278\n",
      "train loss:0.016574597337157285\n",
      "train loss:0.03354043552488864\n",
      "train loss:0.018667878981756744\n",
      "train loss:0.048510825611702973\n",
      "train loss:0.0301883626078954\n",
      "train loss:0.0437188875542261\n",
      "train loss:0.05552399325956854\n",
      "train loss:0.033793573579411054\n",
      "train loss:0.047526298169267685\n",
      "train loss:0.024922295488272673\n",
      "train loss:0.06189987295240042\n",
      "train loss:0.12873678972761068\n",
      "train loss:0.030533484493744333\n",
      "train loss:0.020373925596593064\n",
      "train loss:0.028522558753357456\n",
      "train loss:0.02580366885974312\n",
      "train loss:0.03342711500064102\n",
      "train loss:0.011698023706747319\n",
      "train loss:0.03286100887193835\n",
      "train loss:0.04039682608505468\n",
      "train loss:0.030884260142292567\n",
      "train loss:0.0624110310758667\n",
      "train loss:0.026382482561116537\n",
      "=== epoch:17, train acc:0.986, test acc:0.951 ===\n",
      "train loss:0.029181798219416796\n",
      "train loss:0.07270140750805593\n",
      "train loss:0.0431943980433352\n",
      "train loss:0.04039548380363163\n",
      "train loss:0.020685220710776088\n",
      "train loss:0.053306191361740876\n",
      "train loss:0.026645522039417963\n",
      "train loss:0.02079897726681082\n",
      "train loss:0.023564335780377393\n",
      "train loss:0.01994776565140801\n",
      "train loss:0.03477133819698546\n",
      "train loss:0.07063192125329679\n",
      "train loss:0.07261602717593157\n",
      "train loss:0.048195640101686726\n",
      "train loss:0.03893711509348993\n",
      "train loss:0.04284383828934544\n",
      "train loss:0.02389064397990991\n",
      "train loss:0.019108647009953475\n",
      "train loss:0.14043976608185824\n",
      "train loss:0.011960923309519969\n",
      "train loss:0.05827840888836411\n",
      "train loss:0.1568587404104875\n",
      "train loss:0.053800572415731815\n",
      "train loss:0.030775681218054365\n",
      "train loss:0.03959011145901715\n",
      "train loss:0.05219265014521181\n",
      "train loss:0.05682774257373863\n",
      "train loss:0.050280972458700904\n",
      "train loss:0.06679415562500497\n",
      "train loss:0.07437308656025182\n",
      "train loss:0.038457927224470986\n",
      "train loss:0.024628654099925686\n",
      "train loss:0.007163400320534985\n",
      "train loss:0.026283071292255065\n",
      "train loss:0.07054782839578119\n",
      "train loss:0.020662101611596793\n",
      "train loss:0.019230350198207118\n",
      "train loss:0.023898893424568936\n",
      "train loss:0.0241567294352012\n",
      "train loss:0.028041976979435095\n",
      "train loss:0.023241218138015163\n",
      "train loss:0.022869771748623213\n",
      "train loss:0.013781111752311594\n",
      "train loss:0.04546550704363763\n",
      "train loss:0.023536467960749877\n",
      "train loss:0.01245624367411348\n",
      "train loss:0.06977934239384766\n",
      "train loss:0.03903493935810261\n",
      "train loss:0.04400710055470513\n",
      "train loss:0.013591111081125056\n",
      "=== epoch:18, train acc:0.99, test acc:0.957 ===\n",
      "train loss:0.06986952087620656\n",
      "train loss:0.05736877055758301\n",
      "train loss:0.04654024902264762\n",
      "train loss:0.037287579725366136\n",
      "train loss:0.03028424434956769\n",
      "train loss:0.0484892041905426\n",
      "train loss:0.013515651045761285\n",
      "train loss:0.03234321298605\n",
      "train loss:0.05084560632257987\n",
      "train loss:0.027307134189417947\n",
      "train loss:0.02680525701130708\n",
      "train loss:0.057330719559979676\n",
      "train loss:0.023286553859583357\n",
      "train loss:0.04700345921195889\n",
      "train loss:0.0732995361332501\n",
      "train loss:0.026915131775655014\n",
      "train loss:0.03989528540882829\n",
      "train loss:0.037539423455954755\n",
      "train loss:0.05250456116031069\n",
      "train loss:0.016719726058681342\n",
      "train loss:0.018555396241999783\n",
      "train loss:0.06724407038979345\n",
      "train loss:0.023658004333285625\n",
      "train loss:0.03580155610986468\n",
      "train loss:0.017789178716565525\n",
      "train loss:0.020517972920346086\n",
      "train loss:0.03928641493067496\n",
      "train loss:0.03785209125426527\n",
      "train loss:0.01675611419309601\n",
      "train loss:0.027019565350007574\n",
      "train loss:0.011831568828650562\n",
      "train loss:0.03487141878463005\n",
      "train loss:0.033054933094247126\n",
      "train loss:0.07439646029519446\n",
      "train loss:0.01638580968281467\n",
      "train loss:0.025223721401130172\n",
      "train loss:0.021817786092927064\n",
      "train loss:0.05733899631772366\n",
      "train loss:0.010717881501437918\n",
      "train loss:0.08844432873855218\n",
      "train loss:0.09201135251268794\n",
      "train loss:0.01510373695768146\n",
      "train loss:0.024391386137376302\n",
      "train loss:0.03662900463002247\n",
      "train loss:0.024071685530906554\n",
      "train loss:0.01708648824694813\n",
      "train loss:0.030376391032998627\n",
      "train loss:0.041892112458188316\n",
      "train loss:0.04592120238574195\n",
      "train loss:0.009927330706147618\n",
      "=== epoch:19, train acc:0.992, test acc:0.952 ===\n",
      "train loss:0.01176201026650916\n",
      "train loss:0.025483388411722534\n",
      "train loss:0.024333074542974623\n",
      "train loss:0.05984368389040281\n",
      "train loss:0.022356543540288744\n",
      "train loss:0.042496162900311364\n",
      "train loss:0.023183994212069393\n",
      "train loss:0.013778423377795041\n",
      "train loss:0.03819146361121397\n",
      "train loss:0.04199396278094828\n",
      "train loss:0.010736597237463532\n",
      "train loss:0.018155296526910752\n",
      "train loss:0.033524786162647174\n",
      "train loss:0.03346751185863868\n",
      "train loss:0.040003899578215714\n",
      "train loss:0.03285983760128934\n",
      "train loss:0.06171626835735748\n",
      "train loss:0.016219900649239952\n",
      "train loss:0.0355925346246364\n",
      "train loss:0.016048806264582053\n",
      "train loss:0.02549562863236978\n",
      "train loss:0.07808394089666675\n",
      "train loss:0.030730316759965022\n",
      "train loss:0.05123220537342329\n",
      "train loss:0.01275273266211777\n",
      "train loss:0.019236081889550154\n",
      "train loss:0.04822569181542785\n",
      "train loss:0.03211024729060085\n",
      "train loss:0.041014686131321546\n",
      "train loss:0.01766847543415904\n",
      "train loss:0.014248695654083996\n",
      "train loss:0.026470948406619104\n",
      "train loss:0.025970884447019324\n",
      "train loss:0.07886587563332609\n",
      "train loss:0.029426879106888012\n",
      "train loss:0.05313496358572511\n",
      "train loss:0.025709548928754638\n",
      "train loss:0.05408915163064643\n",
      "train loss:0.009110565371723557\n",
      "train loss:0.022640488109853404\n",
      "train loss:0.014976242120575002\n",
      "train loss:0.021324054336493013\n",
      "train loss:0.052301931660707124\n",
      "train loss:0.010011188903891384\n",
      "train loss:0.014454504763613435\n",
      "train loss:0.0610472365569547\n",
      "train loss:0.01263928718954744\n",
      "train loss:0.019020426297542862\n",
      "train loss:0.009301327692693463\n",
      "train loss:0.017271923364596466\n",
      "=== epoch:20, train acc:0.995, test acc:0.962 ===\n",
      "train loss:0.017521984661753403\n",
      "train loss:0.031146084458640047\n",
      "train loss:0.03198568152578355\n",
      "train loss:0.024517374266394075\n",
      "train loss:0.01036003519099242\n",
      "train loss:0.011492965468129675\n",
      "train loss:0.0342256151999787\n",
      "train loss:0.01338283710606567\n",
      "train loss:0.028675858091352594\n",
      "train loss:0.015264942253238743\n",
      "train loss:0.04376534812402841\n",
      "train loss:0.04704421632349004\n",
      "train loss:0.005805540956936889\n",
      "train loss:0.012822533816556543\n",
      "train loss:0.007895561746133662\n",
      "train loss:0.02684410997721298\n",
      "train loss:0.040539796074812104\n",
      "train loss:0.014176572670853642\n",
      "train loss:0.019447623004716065\n",
      "train loss:0.014180738904134925\n",
      "train loss:0.013174104766760921\n",
      "train loss:0.028573126937918723\n",
      "train loss:0.01815592853750484\n",
      "train loss:0.015484621746937356\n",
      "train loss:0.00974213191316357\n",
      "train loss:0.008877420013071223\n",
      "train loss:0.016089508768266952\n",
      "train loss:0.027511336935292115\n",
      "train loss:0.02405861482707417\n",
      "train loss:0.010633793121515639\n",
      "train loss:0.01409100295072856\n",
      "train loss:0.016114229138524463\n",
      "train loss:0.01355413173679557\n",
      "train loss:0.020925609675785033\n",
      "train loss:0.0098195301038167\n",
      "train loss:0.007941694164381551\n",
      "train loss:0.012068672379492938\n",
      "train loss:0.031712556675161506\n",
      "train loss:0.006547128405430057\n",
      "train loss:0.01756546071344096\n",
      "train loss:0.031797339492546525\n",
      "train loss:0.01484099902808561\n",
      "train loss:0.02402751436930082\n",
      "train loss:0.005403608143290752\n",
      "train loss:0.00842120401183597\n",
      "train loss:0.005337335581110683\n",
      "train loss:0.01516008194434311\n",
      "train loss:0.017024796383178463\n",
      "train loss:0.024912619020337576\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.964\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20 #20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
