{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a380315",
   "metadata": {},
   "source": [
    "### 情報工学工房 第3回レポート課題  \n",
    "\n",
    "\n",
    "#### 課題：課題１～３を以下のとおり実施した。\n",
    "##### 感想：動画等活用して誤差逆伝搬法の概念を理解するのにはそれほど苦労しなかったが、実装の理解には相当苦労した。今のレベルだとサンプルコードのようなスマートで無駄のない配列の受け渡し処理を独力で組むのはほとんど不可能なように思う。調べたり試行錯誤するうちに理解が深まり、力がついているのは実感しているので\n",
    "##### 参考文献：\n",
    "- Snow Tree in June, NumPy♪nditerを使うと様々な次元数に対応できる, https://snowtree-injune.com/2020/06/29/nditer-z009/, 2022-06-08\n",
    "- 予備校のノリで学ぶ「大学の数学・物理」, 絶対に理解させる誤差逆伝播法【深層学習】, https://www.youtube.com/watch?v=0itH0iDO8BE, 2022-06-04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a9160",
   "metadata": {},
   "source": [
    "#### 課題１　Softmax with lossレイヤーを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d29c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5daa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        # forwardの式\n",
    "        # -sum ( t * log (y))\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    def backward(self, dout=1):\n",
    "        # backwardの式\n",
    "        # yi - ti (iはIndex)\n",
    "        batch_size = self.t.shape[0]\n",
    "        # Backwardを実装して、微分値をdxに代入してくださいb\n",
    "        dx = (self.y - self.t) / batch_size \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaba2e6",
   "metadata": {},
   "source": [
    "#### 課題２　Two layer netにおける勾配の確認 \n",
    "Two layer netは教科書の写経を流用した。\n",
    "結果：重み、バイアスいずれも数値微分との差は小さくなった。ただし1層目と2層目で差があり、複数回実施していずれも1層目の方が大きな差となったが、原因特定に至らなかった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bf12a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:1.1326125727475538e-07\n",
      "b1:9.505142234304499e-07\n",
      "W2:9.788660434246944e-13\n",
      "b2:1.201261340399995e-10\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[5:30]\n",
    "t_batch = t_train[5:30]\n",
    "# 数値微分\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "# Backward\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "#grad_backprop = gradient(x_batch, t_batch)\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779855b",
   "metadata": {},
   "source": [
    "   #### Softmax with loss 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90cd007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(network, x, t):\n",
    "    # 自分で実装したSoftmax with lossクラスを使ってみてください\n",
    "    lastLayer = SoftmaxWithLoss()\n",
    "    # forward\n",
    "    #self.loss(x, t)\n",
    "    network.loss(x, t)\n",
    "    # backward\n",
    "    dout = 1\n",
    "    dout = lastLayer.backward(dout)\n",
    "    #layers = list(self.layers.values())\n",
    "    layers = list(network.layers.values())\n",
    "    layers.reverse()\n",
    "    for layer in layers:\n",
    "        dout = layer.backward(dout)\n",
    "    # 設定\n",
    "    grads = {}\n",
    "    #grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "    grads['W1'], grads['b1'] = network.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "    #grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "    grads['W2'], grads['b2'] = network.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e165a",
   "metadata": {},
   "source": [
    "#### 課題３　Two layer net の学習結果\n",
    "lr0.1～3の間で複数試した結果、lr0.3の学習速度が最も速かった。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8e50efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10683333333333334 0.1135\n",
      "0.9305833333333333 0.9289\n",
      "0.9488333333333333 0.9455\n",
      "0.96445 0.9632\n",
      "0.97145 0.965\n",
      "0.9750833333333333 0.9672\n",
      "0.9765333333333334 0.9682\n",
      "0.9798666666666667 0.9709\n",
      "0.9835 0.9726\n",
      "0.9860666666666666 0.9734\n",
      "0.9842166666666666 0.9711\n",
      "0.9856 0.9734\n",
      "0.98775 0.9761\n",
      "0.9875833333333334 0.9724\n",
      "0.9895333333333334 0.9741\n",
      "0.98875 0.9733\n",
      "0.9921333333333333 0.9755\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.3\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    # 勾配\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
